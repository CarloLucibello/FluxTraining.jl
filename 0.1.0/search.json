[{"body":"private   edgesrunafter   —   function Return a vector of  Edge s representing dependencies defined by  runafter .","id":"docstrings/FluxTraining.edgesrunafter.html"},{"body":"1 .  Callback  struct A callback definition has to subtype the abstract  Callback  type .  It can include fields to use as internal state, but we don ’ t need that here .","id":"docs/callbacks/custom.html#callback-struct"},{"body":"private   callbackgraph   —   function Creates a directed acyclic graph from a list of  callbacks . Ordering is given through  runafter  and  resolveconflict . If a write conflict cannot be resolved (i . e .   resolveconflict ) is not implemented), throws an error .","id":"docstrings/FluxTraining.callbackgraph.html"},{"body":"Resolving conflicts There are two methods for resolving conflicts,  runafter  and  resolveconflict . runafter  allows you to define list of callbacks that should run before the callback .  For example,  Recorder  needs to run after all metrics: resolveconflict  provides more granular control to handle a possible conflict between two callbacks .  It takes two callbacks and defines how to resolve a conflict:","id":"docs/callbacks/custom.html#resolving-conflicts"},{"body":"public   fit!   —   function","id":"docstrings/FluxTraining.fit!.html"},{"body":"Changes add default optimizer and use weight decay","id":"docs/status.html#changes"},{"body":"Callbacks Callbacks are a powerful feature of  FluxTraining . jl , allowing you to add functionality to the training loop at different points . Read on if you want to know how to  use callbacks  when training what callbacks come  included with  FluxTraining . jl ; or how callbacks work and how to  implement your own","id":"docs/overview.html#callbacks"},{"body":"Visualize the callback dependency graph You can use  GraphPlot . jl  to visualize the dependencies between callbacks: (the target of an arrow depends on the origin) As an example for a detected dependency, we can see that  Recorder  runs after  Loss .   Recorder  records the values of all metrics, so  Loss  which is a subtype of  AbstractMetric  needs to run first .","id":"docs/callbacks/tipstricks.html#visualize-the-callback-dependency-graph"},{"body":"public   BackwardEnd   —   struct Called between calculating gradients and updating parameters ``` { =html}","id":"docstrings/FluxTraining.Events.BackwardEnd.html"},{"body":"Planned Features","id":"docs/status.html#planned-features"},{"body":"public   Recorder   —   struct","id":"docstrings/FluxTraining.Recorder.html"},{"body":"Conflict resolution When creating a  Learner , a dependency graph is created .  The graph is then analyzed to find possible conflicts (for example, when two callbacks update the same state) . Conflicts are detected automatically and will result in an error .  Conflicts happen when the same state is being modified by multiple callbacks and it is unclear which order of running them (if any) is valid .","id":"docs/callbacks/custom.html#conflict-resolution"},{"body":"public   FitEvent   —   type Abstract type for events that callbacks can hook into","id":"docstrings/FluxTraining.Events.FitEvent.html"},{"body":"Name Module Visibility Category  BackwardBegin   FluxTraining.Events   public   struct   BackwardEnd   FluxTraining.Events   public   struct   FitEvent   FluxTraining.Events   public   type   GradEvent   FluxTraining.Events   public   type   Init   FluxTraining.Events   public   struct   LossBegin   FluxTraining.Events   public   struct   Events   FluxTraining.Events   private   module   GarbageCollect   FluxTraining   public   function   History   FluxTraining   private   struct   HyperParameter   FluxTraining   private   parametric type   Learner   FluxTraining   public   struct   Metric   FluxTraining   public   parametric type   MetricsLogger   FluxTraining   public   struct   Phases   FluxTraining.Phases   private   module   ProgressBarLogger   FluxTraining   public   struct   Recorder   FluxTraining   public   struct   Schedule   FluxTraining   public   parametric type   StopOnNaNLoss   FluxTraining   public   struct   accesses   FluxTraining   private   function   callbackgraph   FluxTraining   private   function   edgesrunafter   FluxTraining   private   function   fit!   FluxTraining   public   function   fitepochphase!   FluxTraining   private   function   on   FluxTraining   private   function   sethyperparameter!   FluxTraining   private   function   stateaccess   FluxTraining   private   function ","id":"docstrings.html#docstring-index"},{"body":"Status FluxTraining . jl  will be ready for a first release soon .  It is part of an ongoing effort to improve Julia ’ s deep learning infrastructure and will be the training library for the work - in - progress  FastAI . jl . Drop by on the  Julia Zulip  and say hello in the stream  #ml-ecosystem-coordination .","id":"README.html#status"},{"body":"Logging and I/O Checkpointer ProgressBarLogger * Recorder * MetricsLogger *","id":"docs/callbacks/reference.html#logging-and-io"},{"body":"private   on   —   function Handle  event  with  callback .  Can dispatch on an  Phase  and receives  learner  as an additional argument . If not overwritten with a more specific method, does nothing . To see events which an  AbstractCallback  handles, use","id":"docstrings/FluxTraining.on.html"},{"body":"4 .  Dependencies Let ’ s improve our callback a bit by adding the current step number to the printed message, so it will look like this:  \"Step 14 loss: 0.0032\" . For that we need to know what the current step number is .  One way to go about this is to add a field to  Printer  that starts at  0  and is incremented every batch . Luckily, there already is a callback that tracks this kind of statistics, the  Recorder .  It uses a special piece of state,  learner.cbstate , to store a  History  with this information . learner.cbstate  is a dictionary where callbacks can store state that they want to make available to other callbacks .  Like any other piece of state, the callback writing to it needs to add a  Write()  permission to it using  stateaccess . What makes  cbstate  special is that when creating the callback graph, it is checked that every entry in  cbstate  that is accessed is being created first . The update to the event handler looks like this: We also need to update the definition of  stateaccess  now: Since  Printer  depends on  Recorder  now, an error will be thrown if you try to use  Printer  without  Recorder . And that ’ s it, pass  Printer  to a  Learner  and test it out !  The upside of jumping through some additional hoops is that using the callback in the wrong context will always result in an error, so the user can have peace of mind .","id":"docs/callbacks/custom.html#dependencies"},{"body":"private   accesses   —   function Enumerate all valid state accesses of permissions of kind  perm . accesses((x = Read(),), Read()) === [(:x,)] accesses((x = Read(),), Write()) === []","id":"docstrings/FluxTraining.accesses.html"},{"body":"private   stateaccess   —   function Defines what  Learner  state is accessed when calling sethyperparameter!  and  gethyperparameter","id":"docstrings/FluxTraining.stateaccess.html"},{"body":"public   MetricsLogger   —   struct Prints any metrics after every epoch .","id":"docstrings/FluxTraining.MetricsLogger.html"},{"body":"Custom callbacks FluxTraining . jl ’ s callback system is built around multiple dispatch, so you specify which part of the training you want to  “ hook into ”  by dispatching on  Phase s and  Event s .","id":"docs/callbacks/custom.html#custom-callbacks"},{"body":"private   Events   —   module","id":"docstrings/FluxTraining.Events.html"},{"body":"FluxTraining . jl Dev A powerful, extensible neural net training library . FluxTraining . jl  gives you an endlessly extensible training loop for deep learning .  It is inspired by  fastai . It exposes a small set of extensible interfaces and uses them to implement hyperparameter scheduling metrics logging training history; and model checkpointing Read  getting started  first and the  user guide  if you want to know more .  See also the  reference  for detailed function documentation .","id":"README.html#fluxtrainingjl"},{"body":"public   StopOnNaNLoss   —   struct Stops the training when a NaN loss is encountered .","id":"docstrings/FluxTraining.StopOnNaNLoss.html"},{"body":"API The following types and functions can be used to create custom callbacks .  Read the  custom callbacks guide  for more context . Callback stateaccess runafter resolveconflict","id":"docs/callbacks/reference.html#api"},{"body":"private   fitepochphase!   —   function Fit  learner  for one epoch . Customize by deriving custom phase from  Phase .","id":"docstrings/FluxTraining.fitepochphase!.html"},{"body":"Listing event handlers for a callback Use  Base.methods  to check what events a callback handles:","id":"docs/callbacks/tipstricks.html#listing-event-handlers-for-a-callback"},{"body":"Using callbacks Callbacks allow injecting functionality at many points during the training loop . To use them, pass a list of callbacks to  Learner : Some useful callbacks are added by default: See  callback reference  for a list of all callbacks included in  FluxTraining . jl  and their documentation . The order the callbacks are passed in doesn ’ t matter .   FluxTraining . jl  creates a dependency graph that makes sure the callbacks are run in the correct order .  Read  custom callbacks  to find out how to create callbacks yourself .","id":"docs/callbacks/usage.html#using-callbacks"},{"body":"3 .  State As seen above, the callback handler  on  receives as the last argument a  Learner  instance, allowing the callback to access and modify state . If we wanted to print the last batch ’ s loss instead of a generic message, we could update our definition of  on : (see  Learner  for in - depth documentation of the  Learner ’ s state) The ability to modify any state is very powerful, but it can quickly become problematic when it is unclear which callbacks modify what state and what the correct order should be . Because of that,  FluxTraining . jl  prevents callbacks from reading and modifying state by default .  If we tried to use the above redefinition of  on , we would get the following error: To fix that error, we need to implement  stateaccess , a function that specifies what state a callback is allowed to read and write . In our case, we want to read the loss of the current batch: (see  stateaccess  for more information on how to implement it) After that definition, the above code will run fine . This might seem bothersome, but this extra information makes it possible to analyze state dependencies before any code is run and saves you from running into nasty, hard - to - find bugs that can occur when using many callbacks together .","id":"docs/callbacks/custom.html#state"},{"body":"Examples Schedule  is a wrapper around  Animations.Animation , see the documentation for more detailed information .","id":"docstrings/FluxTraining.Schedule.html#examples"},{"body":"This page should give you an overview","id":"docs/overview.html"},{"body":"Metrics Loss * Metric","id":"docs/callbacks/reference.html#metrics"},{"body":"private   sethyperparameter!   —   function Sets hyperparameter  H  to  value  on  learner .","id":"docstrings/FluxTraining.sethyperparameter!.html"},{"body":"public   Init   —   struct Called once when the learner is created/the callback is added .","id":"docstrings/FluxTraining.Events.Init.html"},{"body":"Keyword arguments usedefaultcallbacks = true : Whether to add some basic callbacks .  Included are  Loss ,  Recorder ,  ProgressBarLogger , StopOnNaNLoss , and  MetricsLogger cbrunner = LinearRunner() : Callback runner to use .","id":"docstrings/FluxTraining.Learner.html#keyword-arguments"},{"body":"public   ProgressBarLogger   —   struct Prints a progress bar of the currently running epoch .","id":"docstrings/FluxTraining.ProgressBarLogger.html"},{"body":"Execution By default, a a topological ordering of the callbacks is created from the dependency graph and the callbacks are executed serially . This behavior can be overwritten with custom callback executors, for example to create a  Dagger . jl  node from the graph to allow callbacks to safely run in parallel where valid .","id":"docs/callbacks/custom.html#execution"},{"body":"Callback reference","id":"docs/callbacks/reference.html#callback-reference"},{"body":"Arguments model data : Tuple of data iterators in the order  (traindata, valdata, [testdata]) . Must be iterable and return tuples of  (xs, ys) lossfn : Function with signature  lossfn(model(x), y) -> Number optimizer callbacks... : Any other unnamed arguments are callbacks","id":"docstrings/FluxTraining.Learner.html#arguments"},{"body":"A guided example There are 4 things you need to do to implement a custom callback: Create a callback  struct  that subtypes  Callback Write event handlers with  on Define what state the callback accesses by implementing  stateaccess (Optionally) define dependencies on other callbacks with  runafter Let ’ s go through them one at a time by implementing a simple callback that prints something after every batch .","id":"docs/callbacks/custom.html#a-guided-example"},{"body":"Tips  &  tricks","id":"docs/callbacks/tipstricks.html#tips--tricks"},{"body":"Included callbacks FluxTraining . jl  comes included with many callbacks .  Some of them are added to  Learner  by default, here marked with a  * .","id":"docs/callbacks/reference.html#included-callbacks"},{"body":"public   Metric   —   parametric type fn(y_pred, y)","id":"docstrings/FluxTraining.Metric.html"},{"body":"2 .  Event handlers Now we need to add an event handler so that  Printer  can run some code when a batch ends . Event handlers can be defined by adding a method to  FluxTraining.on .  It takes as arguments an  event , a  phase , the callback and the learner: on(event::Event, phase::Phase, callback::Callback, learner) The  event ,  phase  and  callback  are used to dispatch . In this case, we want to run code at the end of a batch, so the event we need to dispatch on is  BatchEnd .  We want it to run in any phase, so we use the abstract type  Phase .  The third argument type is the callback we want to add an event handler to .  This gives us: We can now pass an instance of  Printer  when creating a  Learner  and the message will be printed at the end of every batch .","id":"docs/callbacks/custom.html#event-handlers"},{"body":"Training loop The training loop is","id":"docs/overview.html#training-loop"},{"body":"public   BackwardBegin   —   struct Called between calculating loss and calculating gradients ``` { =html}","id":"docstrings/FluxTraining.Events.BackwardBegin.html"},{"body":"public   Schedule   —   parametric type Describes how the values of a hyperparameter change over the training .","id":"docstrings/FluxTraining.Schedule.html"},{"body":"Ecosystem Unlike fastai,  FluxTraining  focuses on the training part of the deep learning pipeline .  Other packages you may find useful are Metalhead . jl  and  FluxModels . jl  for models Augmentor . jl  and  DataAugmentation . jl  for data augmentation DataLoaders . jl  for parallel data loading; and DLDatasets . jl  for datasets","id":"docs/ecosystem.html#ecosystem"},{"body":"private   History   —   struct epochs::Int64 Default: 0 nsteps::Int64 Default: 0 nstepsepoch::Int64 Default: 0 nsamples::Int64 Default: 0 epochmetrics::DataStructures.DefaultDict Default: DefaultDict ((() - >begin #= /home/runner/work/FluxTraining . jl/FluxTraining . jl/src/callbacks/recorder . jl:17 =# MVHistory() end)) stepmetrics::ValueHistories.MVHistory Default: MVHistory() hyperparams::ValueHistories.MVHistory Default: MVHistory()","id":"docstrings/FluxTraining.History.html"},{"body":"Callbacks ToTorch  callback that uses  Torch.jl  ( )","id":"docs/status.html#callbacks"},{"body":"Training loop EarlyStopping StopOnNaNLoss * Scheduler ToGPU Additionally, some abstract types are defined .  It is recommended that you subtype from these where it makes sense so they will be play nicely with other callbacks . AbstractMetric AbstractLogger","id":"docs/callbacks/reference.html#training-loop"},{"body":"Fields model ,  optimizer , and  lossfn  are stored as passed in data  is a  NamedTuple  of  (training = ..., validation = ..., test = ...) . Some values might be  nothing  if you didn ’ t pass in multiple data iterators . params : an instance of  model ’ s parameters of type  Flux.Params batch : State of the current batch, including: batch.xs : model inputs batch.ys : target outputs batch.ŷs : model outputs, i . e .   model(xs) batch.loss : batch loss, i . e .   lossfn(ŷs, ys) batch.gs : batch gradients, instance of  Zygote.Grads ( ! ) Note: Depending on the progress of the step, some fields may be  nothing , e . g .  the  gs  before the backward pass . cbstate::Dict{Symbol,Any} : Special state container that callbacks can save state to for other callbacks .  Its keys depend on what callbacks are being used .  See the  custom callbacks guide for more info .","id":"docstrings/FluxTraining.Learner.html#fields"},{"body":"Getting started In  FluxTraining , a  Learner  holds all state necessary for training .  To get started, you need a  model training and validation  data iterators a  loss function ; and an  optimizer Models and datasets are not included with FluxTraining . jl, so see  ecosystem  for supporting packages . Let ’ s look at a simple training example . First we define the necessary pieces: Then we construct a  Learner : And train for 10 epochs: Most functionality in  FluxTraining.jl  is implemented as a  callback . Callbacks can add all kinds of functionality by hooking into the training loop .  For example, the following callbacks are used by default when you create a learner: ProgressBarLogger  prints the progress of the current epoch MetricsLogger  prints the metrics of the last epoch StopOnNaNLoss  stops the training when a  NaN  loss is encountered","id":"docs/getting_started.html#getting-started"},{"body":"public   LossBegin   —   struct Called between calculating  y_pred  and calculating loss ``` { =html}","id":"docstrings/FluxTraining.Events.LossBegin.html"},{"body":"public   GradEvent   —   type Supertype for events that are called within  Zygote.gradient . They need to be handled differently because try/catch is not supported by Zygote ’ s compiler .","id":"docstrings/FluxTraining.Events.GradEvent.html"},{"body":"public   GarbageCollect   —   function Every  nsteps  steps, forces garbage collection . Use this if you get memory leaks from, for example, parallel data loading .","id":"docstrings/FluxTraining.GarbageCollect.html"},{"body":"private   Phases   —   module","id":"docstrings/FluxTraining.Phases.html"},{"body":"private   HyperParameter   —   parametric type A hyperparameter is any state that influences the training and is not a parameter of the model . Hyperparameters can be scheduled using the  Scheduler callback .","id":"docstrings/FluxTraining.HyperParameter.html"},{"body":"public   Learner   —   struct Holds and coordinates all state of the training .   model  is trained by optimizing  lossfn  with  optimizer  on  data .","id":"docstrings/FluxTraining.Learner.html"}]